{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Demo\n",
    "\n",
    "This is an early of UCSD's markdown to lecture pipeline through HeyGen. This pipeline takes you from a text document, or script, all the way to a finished video. The way it works is you edit a $\\verb|.md|$ found in the notebook scripts folder and feed the name to the second code cell in this notebook. An example and discussion of the syntax can be found in $\\verb|examples/example.md|$ or $\\verb|syntax.md|$. A list of voice and avatar ids is available in the files $\\verb|avatar_options/voices.txt|$ and $\\verb|avatar_options/avatars.txt|$.\n",
    "\n",
    "Supported syntax:\n",
    "\n",
    "    - Avatar: Sets chacteristics of the avatar: id, voice_id, position, scale, style, cbc and bc. Inovked with ()\n",
    "\n",
    "    - Composition: can set picture in picture or avatar-only composition type. Invoked with []\n",
    "        \n",
    "    - Transition: any transition including concatenation, any duration. Invoked with {}\n",
    "\n",
    "        Ex: {0.5, wipeleft} This will invoke a wipeleft transition that lasts 0.5 seconds\n",
    "\n",
    "To invoke a transition simply introduce a new line or use a {} command. [] or () commands in the middle of the line will trigger a midline cut like so:\n",
    "        \n",
    "    {hlwind, 3.0} [style:circle, background:#F5E3A2, position:(0.75;0.75)]This is a different video. If everything worked, it's been stitched together with the previous video with a visual and audio cross fade. The only clip composition that is currently supported is the picture in picture made by heygen with default a setting of placing the avatar at half scale in the bottom right of the slide. [type: avatar-only] (style:closeUp, position:0.25;0.75) Slides advance in order with the each paragraph.Future versions of this program will expand on these functionalities. I will figure out how to import a PDF instead of a series of images. I will support more complex clip compositions such as side by side, avatar-only, slide-only and [type:pip] (style:normal, position:0.75;0.75)imported media as well as more advanced transitions like sudden changes mid-sentence or wipes and slides or cross dissolves. The markdown syntax will obviously evolve concurrently.\n",
    "\n",
    "    The [type: avatar-only] (style:closeUp, position:0.25;0.75) is invoked without a {} and without a new line. In this case the script grabs the text before the mid-sentence cut (or mid-clip cut) and adds it to the text of the new clip. In this case there are three videos in one paragraph. They all have different compositions but exactly the same text. That means they produce the same words at the same times, meaning we can use the caption files to choose when to cut the clips. Right now I use the caption files made by HeyGen which don't have word level precision. The script tries to find a best match and cut there. For example, the first cut will try to find the closest timestamp to the phrase \"Slides advance in order with\" in the captions and cut there. The videos are then trimmed and concatenated together. This gives gives the video a more organic feel, I think. I do not currently support switching between voices in a midline cut. It does not work without word-level timestamp information and my best solution so far is to load up a heavy openAI model, which isn't fast, efficient or cost-effective for demo purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that HeyGen seems to have caught on to my strategy of generating an arbitrary number of free trial api keys by using Apple's hide my email function. Calls from those APIs made with a jupyter notebook seem to take forever (I mean about 20 minutes) whereas on my laptop they take about 2 minutes. I need a non-trial APi key to keep doing this at scale. Keep that in mind when using the tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run the cell below before anything with ffmpeg. This is how you install ffmpeg on a binder notebook, thanks to [stack overflow](https://stackoverflow.com/questions/72217039/ffmpeg-and-jupyter-notebooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exist = !which ffmpeg\n",
    "if not exist:\n",
    "  !curl https://johnvansickle.com/ffmpeg/releases/ffmpeg-release-amd64-static.tar.xz -o ffmpeg.tar.xz \\\n",
    "     && tar -xf ffmpeg.tar.xz && rm ffmpeg.tar.xz\n",
    "  ffmdir = !find . -iname ffmpeg-*-static\n",
    "  path = %env PATH\n",
    "  path = path + ':' + ffmdir[0]\n",
    "  %env PATH $path\n",
    "print('')\n",
    "!which ffmpeg\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we're ready to get started. Enter the name of the file you wish to use below. I recommend using one of the files in the notebook_scripts folder. I will not being using those API keys so they are less likely to have hit their 5-clips-a-day limit. You can also use any file with your own API key if you have one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"./notebook_scripts/notebook_script5.md\" # \"./notebook_scripts/notebook_script1.md\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf *.mp4 *.jpg *.ass .\n",
    "! mkdir assets\n",
    "from parse import parse_from_file\n",
    "from upload import upload_script, parse_upload_response, get_slides, get_avatar_clips\n",
    "from compose import compose_scenes\n",
    "import sys\n",
    "import urllib.request\n",
    "from transition import transitions\n",
    "import ffmpeg\n",
    "import time\n",
    "\n",
    "script = parse_from_file(filepath)\n",
    "if script:\n",
    "    responses = upload_script(script)\n",
    "\n",
    "    # parse the response content into the scenes - literally just the avatar video ids\n",
    "    script = parse_upload_response(responses, script)\n",
    "\n",
    "    # get the slides \n",
    "    script = get_slides(script, \"./assets/\")\n",
    "\n",
    "    # then go get the links from the videos and download the clips. hopefully they've rendered by now\n",
    "    #time.sleep(1500)\n",
    "    script = get_avatar_clips(script, \"./assets/\")\n",
    "    print(script)\n",
    "\n",
    "    # compose the scenes\n",
    "    script = compose_scenes(script)\n",
    "    # transitions\n",
    "    (script, v, a, v_d, a_d) = transitions(script)\n",
    "    # output video\n",
    "    ffmpeg.output(v,a, script[0][\"Lecture Name\"]+\".mp4\", vcodec=\"h264\", pix_fmt='yuv420p', crf=18, preset=\"veryslow\", **{'b:a': '192k'}).run(overwrite_output=True)\n",
    "    \n",
    "    # presumably response has the URL of the pending video. for each of the clips get the url. for each one, download it.\n",
    "    # can't do this section without higher API limit yet\n",
    "    #print(responses)\n",
    "else:\n",
    "    print(script)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
