v0.01a
- [x] Edit upload script to get the video id
- [x] Get all the slides
- [x] Get the video clips and probe them
- [x] Compose scenes
- [x] Transitions
- [x] Chain transitions programatically
- [x] Test the get avatar clip method

# Roadmap
- [x] v0.01a trial API. 5 clips max. only xfades (indicated by "\n"s). Only pip generated by HeyGen Finished Thu Apr 25
- [x] v0.01b Write script object in intermediate point to .json and read it back so we can pick up at any point in the process. Change downloaded asset destination folder. Basic error flagging (stop it with the "yo error with", actually print the response text) Fri Apr 26
- [x] v0.01b0 Add tester API keys. 5 of them. Mon Apr 29 /touch up Tue Apr 30
- [x] v0.01b1 Error out when there's an api trial limit excption instead of waiting. (small change) Tue Apr 30
- [x] Decide on markup syntax and supported interactions, as well as complexity levels Wed May 1
- [x] v0.02 Non-fade transitions and different durations. Mon May 6
Monday: Test concat [tested] and long videos using the new format and syntax [tested]
Let's make sure the notebook works and publish the versioning [done]
- [x] v0.02a We're gonna clean it up. Move the folders around and remove the .json files. Done Mon May 6, Uploaded Tues May 7
- [x] v0.03 Support correctly formatted hey gen pip
Wednesday May 8 Log - missing comprehensive testing and jupyter notebook compatibility test but I think it works. Also need to figure out how the background strings work so I can test that and add it in. Finished Thu May 9

- [x] v0.04 Support avatar-only compositions (just to have some variety) and add the mid clip composition change

Sub tasks for v0.04b:
    - support distinguishing between {}/[] commands at the start of a line and anywhere else [x] May 10
    pretty sure everything works with midclip composition change but I need to be sure. May 14th will be spent testing and updating the notebook for a version release.
    - fix missing bool object not being subscriptable [that was an error I believe. As in error with the input]


v0.05 
Redesign key words and syntax definition. Particularly the distinction between avatar_position as in where the avatar clip goes in the larger composition and avatar's position as in where the avatar goes in its own clip. Same for scale and a few other parameters. I will consider using avatar as a sub-object of composition commands but then my parsing logic gets a little messy. Adding another type of command also gets messy because it's more stuff that the user has to keep track of and more stuff that I have to cross check.

double check the syntax parsing
- [x] correct the midline clip adjustments
- [x] remove redundant headers
adjust documentation and examples
- [x] verify the integrity of the restore intermediate now
- [x] fix issue with second voice not playing nice with the transition out now a bug issue
- [x] also why is midline cut getting default transition, should be concat twas a parse issue its ok 
- [x] test same voice different avatar midline cuts totally works

NOTE: midline cuts with the same voice ID seem to work fine. Changing the voice id midclip is not recommended or supported. yet

####### Old roadmap
- [ ] v0.02     Support non-heygen picture-in-picture
- [ ] v0.03     Support side by side compositions
- [ ] v0.04     Support avatar only compositions
- [ ] v0.05     Support media only compositions
- [ ] v0.05a    Support external video compositions (HARD) (scrap this for now (I've forgotten what it meant))
- [ ] v0.06     Support non-xfade transitions
- [ ] v0.07     Support mid-clip transitions
- [ ] v0.07LTS  (Very randomly) this seems like a good place to stop and reassess, and add validation to the script. Why flood HeyGen with incorrect or unsupported API calls. Plus, if I can get to this stage the software will support a ton of stuff already, it may be time to open beta it or start thinking about a VSCode syntax extension-type thing

####### Misc:
- [ ] Expand to include avatar voice id and id in a scene. For demo purposes let's stick to one video-wide avatar and voice.
- [ ] Small extension to include composed clip probe data in a seperate space to the avatar video probe data. JIC we do more complicated operations.
- [ ] Come up with a neater way to handle transitions. Currently the method takes a buuuuunch of parameters because I'm not too familiar with the ffmpeg python wrapper and I haven't well defined my scope for the function. This is an important to-do.
- [ ] Support slide transitions without transitioning the avatar, particularly for side by side clips.
- [ ] Support PDF slides
- [ ] Support file slides
- [ ] Support non slide-type media for background (heygen has a video background format, and I know I could do it too)
- [ ] Add word-level caption generation using whisper AI. For now we do best match and call it  day
- [ ] Add option for before or after midline cut point (as in include the key phrase in the pre or the post clip)
- [ ] Try to optimize all the reading and writing to and from files
- [ ] Optimize the messy lookahead/lookback code the midline cuts stuff in compose.py
- [ ] Consider adding mid-line cut capabilities to clips with transitions
- [ ] Consider not using the same exact text for midclip cuts so that we can get avatars in different sections of their animation loop, which would help disguise the fact that such a loop exists. First try it with best match but I think it will need word-level caption information, which opens up a whole different can of worms with openAI's whisper model