v0.01a
- [x] Edit upload script to get the video id
- [x] Get all the slides
- [x] Get the video clips and probe them
- [x] Compose scenes
- [x] Transitions
- [x] Chain transitions programatically
- [x] Test the get avatar clip method

# Roadmap
- [x] v0.01a trial API. 5 clips max. only xfades (indicated by "\n"s). Only pip generated by HeyGen Finished Thu Apr 25
- [x] v0.01b Write script object in intermediate point to .json and read it back so we can pick up at any point in the process. Change downloaded asset destination folder. Basic error flagging (stop it with the "yo error with", actually print the response text) Fri Apr 26
- [ ] v0.01c Decide on markup syntax and supported interactions, as well as complexity levels
- [ ] v0.02     Support non-heygen picture-in-picture
- [ ] v0.03     Support side by side compositions
- [ ] v0.04     Support avatar only compositions
- [ ] v0.05     Support media only compositions
- [ ] v0.05a    Support external video compositions (HARD)
- [ ] v0.06     Support non-xfade transitions
- [ ] v0.07     Support mid-clip transitions
- [ ] v0.07LTS  (Very randomly) this seems like a good place to stop and reassess, and add validation to the script. Why flood HeyGen with incorrect or unsupported API calls. Plus, if I can get to this stage the software will support a ton of stuff already, it may be time to open beta it or start thinking about a VSCode syntax extension-type thing

Misc:
- [ ] Expand to include avatar voice id and id in a scene. For demo purposes let's stick to one video-wide avatar and voice
- [ ] Small extension to include composed clip probe data in a seperate space to the avatar video probe data. JIC we do more complicated operations.
- [ ] Come up with a neater way to handle transitions. Currently the method takes a buuuuunch of parameters because I'm not too familiar with the ffmpeg python wrapper and I haven't well defined my scope for the function. This is an important to-do