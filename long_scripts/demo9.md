Name: Arturo Amaya
Lecture Name: Lecture 10
HeyGen API key: OTUzNTA4ZTI1NDk2NGU4ZmFhNTE0MzJlY2Q1ZTk1NjUtMTcxMzU2MTc1OA==
Default Composition: [type:pip]
Default Transition: {type: fade, duration: 1.0}
Default Avatar: (id:Luke_public_3_20240306, voice_id:5dddee02307b4f49a17c123c120a60ca, position:0.75;0.75, scale:0.5, style:closeUp, cbc:#453423, bc:#FFEE22)
Slides:
    https://drive.google.com/file/d/1nTGZ4mk4_Ng-QzpJKf6Hq32sQAY6KhZh/view?usp=sharing
    https://drive.google.com/file/d/1XlRuRl_BlZFBErR3HxOOK1eOnDEMnZ90/view?usp=sharing
    https://drive.google.com/file/d/1k7uIUe-ytPmoqHv2n8S7DQFPzazUUgWu/view?usp=sharing
    https://drive.google.com/file/d/1k7uIUe-ytPmoqHv2n8S7DQFPzazUUgWu/view?usp=sharing
    https://drive.google.com/file/d/1rfVPUN-Mdvq2yeuD_EbeF7t2cgip7NkP/view?usp=sharing
--

(position: 0.75;0.25, scale:0.4, style: circle, cbc: #FFFFFF) So here's our cash types. We have capacity kind of misses that can happen. Capacity misses can happen in all of them. Coflict misses can only happen in set associated and direct map, and cold misses again, all cash is susceptible to that. Or cold or compulsory. This is some old data. So when we have to choose something to victimize, we have to have a replacement policy. The replacement policy and direct map is super easy. We just replace the set that we indexed. Says fully associative, we have to decide on who to get rid of. Ideally, you'd like to keep the cache item that is the next use. You have no way to know that. But if I'm loading this value A here and sometime 100 cycles later, I'm going to load A again. I'd like to keep A in the cache. But I don't know that because I don't know if I'm going to load a again. Ps have very dynamic behavior as we seem like branch prediction, we don't really know for sure, where the program is going to go. We do some other things like a recently used. Let's say I have a two way set of set of cash and I keep track of which data was least recently used or which data was most recently used, and evict the one that's least recently used. If I used it recently, I want to keep it. It's probably hot data. If I use it at least recently, it's probably maybe I'm done with it. I can get rid of it. I show you a pseudo LRU algorithm.

{pixelize} (position: 0.85;0.25, scale:0.3, style: circle, cbc: #FFFFFF) Random is easy. We just have a random number generator, really truly random, some pseudo random number generator, and we can see this is pretty old data, but for reasonable cast size big cast sizes True LRU and random aren't that different in performance like here, 2506 kilobytg 1.15% miss rate versus 1.17 for random. For smaller cases. Actually in this case, random is better. I'm sorry, random is worse. There was one case of random is better, wasn't it? No. Here's random, the same as LRU. These are pretty small numbers, and spending a lot of logic on LRU for our 37 way set associative cash. I don't remember, but if we had some kind of PFO, we didn't do LRU was too expensive. But it didn't matter that much. In fact, on the early MIPS machines, we'll talk about TLBs TLB is another kind of cash cash is addresses. Our replacement algorithm for fully associated TLB was random. We just randomly picked an int to replace. 

{coverup} (position: 0.85;0.25, scale:0.3, style: circle, cbc: #FFFFFF) So how do we do LRU seems to be a good way to go. Let's assume that associativity is the number of ways N, and we need to store all possible orderings of least recently used to most recently used, each represented by log two N.  So if I have n ways, then the way I specify a particular way is by number. So I eight ways, then I need three bits to specify each way, zero seven.  If I have eight ways and I want to keep a queue of them from most recently used, I could have a little q. The most really use three bit entry, we go here, least recently use three bit entry we go here. When everybody gets used, they go to the top of the list, everybody pushes down. Last really use will be the last one. That's one way you can implement it. Now, in theory, If you look at the number of permutations of orderings, you could say, well, if there's n of these, then n factorial is the number of permutations of possible orderings. And log two of that number, sealing of log two would be the number of possible orderings.

{radial} (position: 0.85;0.25, scale:0.3, style: circle, cbc: #FFFFFF) And that's the number of orderings for each set, and then I have to do this for every set. This what I described here, this FIFO would be for every set. It's not a trivial amount of logic, and we first started designing these circuits like our 4,000 time frame, it was way too much logic to consider doing. Nowadays, you could probably throw this into Vg come up with some state machine that does this with a reasonable number of gates because we have so many gates now, you can actually compute this. Now it turns out this is not that practical because I think had an example in my notes. If you take a I factorial is 40,320. So there's 40,320 different orderings of these eight things. Then to know which one is least tu used, you'd have to decode each of those permutations, which is least used. That's quite a bit of logic. So it's actually fewer bits. If you do log two factorial times number of sets. It's fewer bits, but you have a lot of combinatorial logic to decode it. This is actually a little simpler. It's a little more bits in this case. This would be eight times log two this is three bits. So three times eight would be 24 bits. Was I think this was 16 bits. Anyway. So there's some trade off to make. Are any questions on that? All right.  Okay . Yeah, this is just a math I just went through. It's 288 ways times three is 24 bits per set. The number of blocks would be 64 k divided by 64 the block side 2024. The number of sets is 120004/8, it's 128 sets, so I need about 303,000 bits or about 384 bytes, which isn't too bad for 64 kilobyte cache overhead. As long as I can do it quickly. Now that may be the challenge. How do you do this quickly?  Okay 

Here are the complexities. When you have a cache miss, I have to read the LRU bits and pick the LRU set B I need to know who am I going to replace. On the right on the refill, when the data comes back, I need to update the LRU bits Because now this lease use things is now the most recently used thing because I've just refilled it. It's the freshest thing. Of course, if you have a machine that's doing multiple loads and multiple cache misses at the same time, this gets fairly complicated. Now, this is  Okay because on a cache miss, I usually have some time. Once I determine it's a mess, everything stopped for a little while. I got to go issue the request to the next level of memory hierarchy, take some cycles. I can do all this computation. On a cache hit, I have to read the LRU bits and update the LRU bits so it's not the least recently used anymore. It's not the most recently used. Now that's problematic. Because in our pipeline, we're either reading the data cache or writing the cache, we're not reading writing the data cache in the same cycle. This could be a problem. It might be a bandwidth issue.  Okay The way you solve this is with a bunch of logic, and as a designer, when you're designing these circuits, you have to think about well, do I store these bits in ram? If I store in ram then the ram has to be dual ported. I have to have enough bamleh to keep up with reason writes on cache hits, or do I do it in discrete logic and just a whole bunch of big placing route block, and it does this with random flip flops and things like that. For 384 bytes, it might be perfectly fine to just do it in discrete logic instead of trying to build a special ram.
