Name: Arturo Amaya
Lecture Name: Lecture 10
HeyGen API key: YmY5NTBiMTYzNDI0NDcxYjliMzBhZTEzM2VlNTFlMGQtMTcxMzU2MTE2MQ==
Default Composition: [type:pip]
Default Transition: {type: fade, duration: 1.0}
Default Avatar: (id:Luke_public_3_20240306, voice_id:5dddee02307b4f49a17c123c120a60ca, position:0.75;0.75, scale:0.5, style:closeUp, cbc:#453423, bc:#FFEE22)
Slides:
    https://drive.google.com/file/d/1nxThwly38ZpnZkq-NjC6RhnzmmL_efaY/view?usp=sharing
    https://drive.google.com/file/d/1bjdHy7xIXpv_5_naAXVo9SvdOrRmO18H/view?usp=sharing
--

So I want to figure out what the number of blocks is first. And then I want to figure out the number sets. And that will determine how big the index is, and then I can figure out how big the tag is based on what's left over. Talk to your neighbors to compare answers. You're all over the map, so pretty even distribution. Art.  Okay. All right. It's looking a little better. Not quite right yet, but we still thinking? Maybe I made a mistake. Definitely possible. I'm going to stop the poll. Let's do it together. See. So right now, we're kind of pretty evenly distributed. All right.  Okay. What is the number of blocks? Well, let's figure that out. So it's four kilobyte. Divided by 32 bytes per block, right? So four k is two 12 and 32 is two to five. I can do 12 minus five to get the number of bits or two 12 minus five, which 12 minus five is seven, right? So that's 12827 is 1208. It's 128 blocks, right? How many sets are there? 64, right? Yeah, divide by two. So it's a 128/2 is 64 sets. So how many bits do I need in the index? Six. How many bits I need in the offset? Five. And this is 32, right? So 32 minus six minus five is 32 -11, which is 21, it's 20 this will be 21 bits. Do you see relationship between the associativity and the size of the tag? What happens if I increase the associativity, what would happen to the size of the tag. For the same size cache. Increase. That makes sense, right because let's say a fully associated cache is going to have zero index bits, which means that the tag is going to be all the way down to here. A direct map cache is going to have one more index bit and fewer tag bits. The complexity of the compare I have to do increases with the associativity. The tag gets wider. The wider the tag is the longer it takes to compare all the bits.

{wipeup}[type: avatar-only] (position: 0.5;0.55, scale:1.0, style: normal, bc: #F88379) What's the average associate, that's a great question. What's the average associated on a cache? I think it depends on which processor you're looking at. I used to think if you just look at Intel, AMD, mainstream processors, they're usually like two or four ways associated, the Zen core showed you before Z four, I think they're four ways associated. But when I started working at Cavium, our catch was 37 way side associate. It's crazy weird number. 37 person is a prime number. I don't know why they choose that. Secondly, it's not power of two, it's all big number. I think it depends. But mainstream processors, there's reasons for this. We'll talk about virtual and physical caches that they're usually 2-8 associative. I think an exception. I just looking the other day. I think the M one, the Mac M has 192 k instruction cache, like a gigantic e cache. It's probably fairly set associative, although I don't know if I know how set associated is. So when we have a miss, we have to decide who to replace. That's called the cache victim. When I have a direct map cache, it's super easy because there's only one set to replace. Only one set and the set only has one way. In a fully associate case, I could replace any of the blocks, and a set associated cache, I replaced any of the blocks that have the same set index. If a cash miss occurs on cash line x and x was previously evicted, this is called the conflict mix. Is that right? This occurs on cash line X. X was previously evicted. Yes. The reason the reason I'm removing cash line x is because I want to make room for something else. And the reason I'm removing is because there's a conflict, right? Because I don't have enough associativity to accommodate it. 

(scale: 0.4) Oh, there's another clicker. Let's just quick. I got to find the start the poll. Let's do like 1 minute. So what's the relative frequency of conflict misses in general from least to most in these caches, assume they're all the same size.  Anyone else want to click in? Question. ðŸ•“ðŸ•“ðŸ•“ðŸ•“ðŸ•“ðŸ•“ðŸ•“ðŸ•“ðŸ•“ðŸ•“ðŸ•“ðŸ•“ So mine previously the first timeline. That makes it a cold mess. Yeah. So if cash line x is the first time I'm missing on it, then it's compulsory. Every cash will miss on it. Does that make sense?  Okay. So I think I'll stop the poll here. So, the answer is D, because I have the most I have the fewest complex misses of fully associative, and then I go set associative, I've reduced the complet misses, and then if I direct map, I have the most complex misses because there's only one way to put something.  Okay 

{vertopen} [type:avatar-only] (position:0.5;0.6, scale: 1.0, style: normal, bc: #FFFFFF) Just an aside. Remember in the last homework or maybe it was slides in the homework, I showed you the R 4,000 pipeline and it had a DF DS and a TC stage. Somebody asked me, what's the TC stage? That is what we do for tag check. The R 4,000 had direct map caches. One of the advantages of a direct map cache in the microarchitecture sense is that I don't need to complete the tag check before I start using the data. Why is that different from set associate? Is set associative cache, I've got to determine which way hit. Then I had that Mux, I have a three way set associate cache. I do all the compares and I select the data I want.  In a direct map cache, I can just start using the data before I even know if it's a hit or not, and that's what we used the TC stage for. So we actually started using the data in DS before you even know if the cache hit or not. And then sometime during the TC stage, we determined it was a hit or miss. I was a miss, then we backed everything up. And that's another ball of wax. But that machine when we was in order machine when we detected a miss sometimes, it was too late. 

{hblur} [type:avatar-only] (position: 0.5;0.6, scale: 1.0, style: normal, bc:#FF4534) The instruction had already moved forward. And so we actually had to back the pipeline backward. Yeah, I guess the advantage of direct cash is we can start using the day before we know if it's good or not. Generally, the cashes work pretty well, and so most of the time it will be a hit, and so it's a good gamble to make. However, direct cash is that we see don't have as good data pattern. Conflict. They have more conflict misses and so our miss rate is going to be worse than it would be with set of soot. There's definitely trade offs to be made. There's other machines that did the same thing. I think in the Alpha, they had a two way set of soga cash and they had a way prediction mechanism. Before they did the tag check, they say, Well, it's probably this way based on history. Just choose way one or way zero and try to get around it that way. In the in the avian machine where we have 37 way set associated cash. The reason I was told that we had that architecture was because it actually saved power because we only had to fire up the way that hit and it worked pretty well for high cash, I suppose.