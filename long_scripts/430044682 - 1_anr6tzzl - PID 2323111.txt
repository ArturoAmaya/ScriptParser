Okay. So today's computer history is about the floppy disc. So in my computer career, let's see. When I started using computers when I was in elementary school or middle school, I guess elementary school I was using a time sharing system. I don't know what they stored things on. I assume it was giant diss and also magnetic tape. When I got my first personal computer when I was in middle school, I think, maybe it was high school. I didn't have a way to store programs away I stored programs or I didn't have a convenient way. The way I stored them was on cassette tape. Cassette tape was a little cartridge about this big predecessor to a CD. It was a magnetic tape it had two little reels in it and a little motor and the tape would go past had anybody seen a cassette tape? Okay, people have seen them. We used to store programs on them and they would just modulate two tones. I think it was 600 hertz and 300 hertz or it was three kill hertz and six kill hertz. Basically two tones. The record two tones. One tone would be a zero, the other tone would be a one. When you wanted to save your program, you set your cassette recorder to record and you played these tones through a wire that your computer was hooked to and it would beep high and low tones for ones and zeros. Then when you wanted to load your program back in, you turn your computer on and you would play the tape recorder into the computer and it would load bit by bit your program into memory, and it was pretty error prone. It was very difficult to find where you stored a program on a tape. Obviously, tape is sequential access. If you want to store multiplegrams on the tape, you'd put them in different posts on the tape, you'd have a little mechanical counter I told you how far into tap it was. It was very painful. Along came these floppy discs a little while later, which were basically magnetic media, but on a piece of mylar plastic. They came and I had one at home. I forgot to bring it. The typical one that first came out, they were 8 " then they had five and a quarter inches of five and a quarter inch from an apple two, and just a flexible disc. It was in a little sleeve of plastic sleeve and inside the sleeve was a circular piece of Molar. I would look like this. Have a little cutout here, which is a right protect tab. If you punch that hole, there was a optical center which prevented the disc from being written, so you could have read only floppies if you want. And then in this sleeve was kind of a slot like that and a hole here. This slot enabled the read head to go back and forth and read different parts of the disc. This hole enabled the motor to spin the disc around There was another hole in the disc here, and that was used by a light sensor, which would know when the hole passed the light sensor, so we know where track one began or sector one. The disc would rotate at certain speed. Let's say the light sensor was here, the disk drive would detect, the disc is now at sector one, certain time later. The thing you wanted to read, let's say in sector four was below the read head and would read it. Use that light sensor to figure out where on the disc because the disc is divided, if you know this. Even today, dis circular, at least the mechanical spinning kind, and they're divided into sectors. Then each of these sectors is divided into tracks. The read right head would move back and forth radially to seek a track called seeking. Then it would wait for the sector that I wanted to read to happen to be below it and then it would read and write. It's pretty amazing it actually worked considering how much mechanical stuff has to go on and even distrib today, the cheapest kind you can buy, the rotating media kind still work this way. It's just that they're sealed up. We went from this flexible one to a semi rigid one. This is probably about in the 1990s. These were three and a quarter inch floppy disc. They're just smaller and they had a hard case, so you couldn't bend them. And that's like the original Mcintosh used that. And then now floppy discs are just gone. You might find them in garage sales. But you're pretty hard pressed probably to find a floppy disc drive to read them. So that's. That's actually something you always have to worry about when you're archiving storage is the thing you're archiving to something that will exist in the future. There's lots of things like that examples of maybe your parents have old VHS tapes from a VCR, actually pretty hard to read because you still have to have a VHS player to read them, similar with fp. I can't use my laptop anymore because someone's piled apple juice on it. It's actually a true story. It was a citta C attack. Daughter was a sophomore in college. She was at the dining hall and she spilled apple juice on her Apple computer and got it all sticky and it wouldn't boot. She took it to the on campus not certified Apple store. I guess if you took it to Apple store, they basically said, Oh, you need a new computer, you have to replace the motherboard and it's going to cost $500 and new computer is $1,700 or something like that. But she took it to the bookstore and they said, Hey, did you spill something on this? And she said, Well, it was apple juice and it was all sticky on the inside. Anyway, they just cleaned it up and it worked fine for the next couple of years. It was a cider attack. Okay. All right. We're talking about memory system. So we talked about pipelines, brands prediction, now we're going to start talking about memory systems a little bit out of order. I want to do memory systems first and then I'm going to come back and do instruction level parasm superscalar. I think it's important to understand how the memory system of computer works and how it affects the performance of the machine. Then we can start looking at some advanced pipeline stuff. Okay. So this is a classic graph. I actually haven't found it updated one, but things really haven't changed in the last ten years 15 years. This is a log graph. The speed here is performance. And performance we're going to measure in this case as latency. So when I talk about processor, I talk about latency, that means how long does it take to solve a problem. So my ability to solve a problem went from a factor of one, By 1990, I could do it ten times faster by 1995, I could do it 100 times faster, and I kept going up and up and up and it still goes up today. This is processor trends, if you looked at something like GPUs or massively parallel processors, you see that this graphic isn't a steeper slope. The amount of computation we have today because of the number transistors because of mores law, we can just keep stamping out more and more computation units and just do more and more computations of performance going up. Memory does not has not been improving the same way. Now, you might ask, well, doesn't Moore's law apply to memory as well does to processors? Does it? Does Moore's law apply to like memory technology? Yeah. You go back and more process, exactly. The answer is, yeah, more law applies to memory devices too. Memory devices, the ones we use today, most of our computers have made of DRM, which is built on semic process, same sem process, similar sems that we build logic on directly optimized for storage. But the same kind of lithography applies, the same kind of processing equipment is used, the same kind of chemicals are used, so forth. Why is it that memory performance isn't improving latency isn't improving. Okay. Because overhead. What are some why is computation improving and memories not? Yeah. We one reason is the wires aren't getting faster. That's true both actually for computation chips and memory chips. Wires aren't getting any faster. Why is memory in particular, it's performance in terms of latency, not improving versus processor performance improving? Well, what makes the processor go. Go ahead. To queens. So there is amount of device physics involved. That's true in terms of as the devices get really small. It's harder, for example, to store charge, you know, like in a Dra. We've kind of overcome some of the things by doing some crazy things in a fab Yeah.'s gotten bigger. Memory has gotten bigger. Yeah, you're onto something. Memory has gotten bigger, so it's harder to access an individual cell. There's actually actually both things are happening and we're just benefiting in different ways. The reason we're getting this crazy performance increase is not because the transistors are going faster, it's because they're smaller, and so we can put more computation units on a die. If you look at a GPU from ten years ago, or let's say 15 years ago, to now, the number of multiple ad units, for example, has gone up exponentially. If I just look at the raw compute capacity of a new GPU, it's a lot faster, a lot more capacity capability than the previous one. Now, I could say the same thing about the memory. The memory, the amount of storage I have now has exponentially expanded compared to what I had ten years ago. I'm actually using More's lot in exactly the same way. It's just that I can take advantage of having more computational units to go faster computationally. I can take advantage of having more memory to have more memory. But it's not faster. So it's a optimizing for different things. Memory is taking advantage of sorry, computer is taking advantage of More's law by having more capability and memory is taking care of getting more transistors to have more storage, right? So we're both actually using More's law. It's just that we're benefiting in different ways. And that's the way it is, right? Because to talk to memory, we need to do communication has been pointed out, the bigger the array, the more communication we have to do to get to individual cell, the more you have There's always a trade off in memory, right? When you want to make memory fast, you make very small arrays with a lot of decoders and a lot of sense amps and all this crazy stuff. If you want to make the memory dense, you make bigger arrays, and now it takes longer to decode the address, it takes longer to sense the wires, it takes longer, everything takes longer. Okay. So this gap has been increasing. If you look at back at processors in 1980, they had some examples that basically didn't have to have cash memory because the processor runs at a certain rate, the memory runs a certain rate. When you go to access memory takes a couple of cycles, you get the data back. Then as this gap started to get bigger and the processors got faster, and the memory latency didn't improve, then we start to have to integrate faster memory. Smaller memories that are faster,s always a trade off, the smaller the memory the faster, the bigger memory, the slower. I can create these caches, which are smaller and take advantage of things we're going to talk about hopefully it's review of fast memory, keeping things close to the processor and only accessing slow memory when I need to. 1980, no integrated cache. RMS thousand circa 2000 was the first chip to have integrated L one and L two. We put L one and L two caches on the chip because of this increasing gap. If it here in 2000, the gap is already getting pretty big. Then I'll show you examples of other processors that do even more. Some basics with caches. Hopefully, this is a review. I'm going to go a little fast, please ask questions or stop me if I'm going too fast because we've all some of different computer architecture background. Compute one fast memory and physics makes bigger memory is slower. That's just the way the physics we live in. Bigger memory means longer wires, wires are slow. Smaller transistors get high density. For example, if I want really high density, use dynamic random axis memory where each bit is stored basically in one transistor or one capacitor. How many are familiar with DRM? D RAM is the memory that you have mostly in your computer. It needs to be refreshed. Every now and then, every now and then every couple of milliseconds or so, your computer is actually reading every cell and refreshing its charge because if it didn't do that, the charge would leak away and you lose data. Also, when you turn the power off, you lose all data because the charges leaks away. Rams are built for multiple transistors. If you took one 40 from me, you've seen these 60 cells cross coupled inverters with a couple of axis transistors, significantly bigger than one transistor. This is not as dense but faster. Architects create hierarchies of memory to trade off speed and size. If I want lots of size, I'm going to have a slow memory, and if I want speed, I'm going to have a smaller memory that's faster. Here's the hierarchies we have and lots of examples, but here's this one. The fastest memory in a computer is usually the register file. That's usually accessible in one cycle. Not all cases, but most machines can access reg one cycle as we've seen in our pipeline. The L one cache and modern machines is typically about four cycles of latency. Then L two is typically like 12 cycles, L three might be like 60 to 100 cycles, and DRM might be anywhere from hundreds of cycles. This could be DDR four, could be HBM memory. It's still significantly slower in latency. The other thing that happens is the bam width So with high performance with low latency, I get a lot of bamwidthRgister file has an extreme amount of bamwid. As we'll see when we do out of order or superscalar machines. Some of these register files have 12 or 15 things I can read in one cycle. So there's a lot of data, and I can similarly I can write five or six things in the same cycle. L one cache typically has an order of gigabytes per second of bandwidth. Then the bandwidth decreases as we go down the hierarchy here. That makes sense. Going further and further away, wires get more expensive, so I'm not going to be able to afford as many wires to get there. Equation change a little bit with HPM with HPM memory, which is die attached on top of the die. Typically they can't afford to have wider interfaces. But generally, that's a trick. You can see all this stuff if you just do CT PxyPU info on your computer, you'll see what kind of computer you have and you can look up all of the different characteristics of how many caches it has and what size they are, stuff like that. Okay. A bit of history. Here's an evolution of caches. This was the first Mctshme out in 1984. I might have told this story. This is the first machine, not the first machine, but the first commercially successful machine that integrated desktop graphics. Stuff, we assume take for granted today, Window managers, point and click interfaces, mice. This is the first machine to really productize that. It was actually developed at Zerox Park. Steve Jobs went there, got these great ideas from Zerox basically integrated them the Apple and Zerox was never able to capitalize off it. Um, just reading this morning, actually. So actually, the first Windows operating system came out before the MAC. When Bill Gates was down at Apple and Steve Job was showing the Mac. At that time, Microsoft and Apple are actually pretty close together because Microsoft was writing software and they wanted Microsoft to write software for the Mac. So they showed him the mac. Bill Gates says, that's amazing. But before they ink the deal between Microsoft and Apple to write software for the Mac, Steve Jobs said, Well, I want to make sure that we have a head start. I want to if you sign this contract, then you cannot develop a graphical user interface for window for PCs until one year after the machine ships. At the time, Steve Jobs like, Oh, this machine is going to ship in 1981. We have exclusive rights on this desktop metaphor, windows, mice, all that stuff until 1982. This machine did actually ship until 1984 because Steve Jobs was wrong about the schedule, and the contract said, 1982, Windows actually came out before this, even though it wasn't as good. And according to the book I'm reading Steve Jobs was furious. But there's nothing you could do because that's what he signed the deal. Anyway. Actually, the MAC interface was much better than Windows 1.0. If you ever go back, you probably find out Internet somewhere windows looked like. It was pretty bad. Okay. Okay. Apple Macintosh, 1984, no cash. It had a 68,000 microprocessor right here. I talked to random access memory, and I talked to de read only memory. The operating system was actually read only memory. When you boot the computer up, it would come up pretty fast. You didn't have to stick a floppy in there. You didn't have to have a disk drive. Most of the critical routines ran out of read only memory. Okay. So the next machine we look at is the Sn one. This is a Sn two, I can't remember. S two. Son came out of a project at Stanford, stands for Stanford University Network. It was started by Andy Bekelhin when he was a grad student, and he started this company called Sn Microsystems along with Bill Joy from Berkeley, who wrote BSDUix and also VI If you use VM that was Bill Joy. Anyway, they started this company. They came on this computer. And it used the same processor as the Mac. It actually had a 68,000 processor. But because of the work station wanted to go fast, they built a unified cache outside the chip made of static ram. We'll talk about unified caches and was virtually addressed, there's all sorts interesting things about it. Anyway, that was about 1985, so only a year after the MAC. This machine costs quite a bit more than the Mac, and MAC costs $400. This is probably like $10,000, which was cheap enough for engineers to get at their work, but not cheap enough that you buy it for your home. Okay. And then we go to something like fairly recent This is AMD Zen four, which they just announced the Zen five. So it's got 1 megabyte. I think the cash I just showed you in the S one, S two was probably 16 k. Might have been 64 k. It was pretty big actually for the time because it was an expensive machine. But this machine has 1 megabyte of L two, 32 megabytes of L three. I mean, the original Mac had 64 kilobytes of main memory. It's main memory is smaller than the L three cache of a modern processor. Then it's got IND cache 32 k. There's a bug on this slide. It says load, it's going this way, stores going that way, but I think those arrows are wrong. I guess this is AMD marketing. Anyway. There's a bunch of details here, which we'll talk about later. Okay, what is the principle behind all these caches? The principle is something called memory locality. That's the principle that a future memory access are near past accesses, and they can be near in two ways. They can be near in time. That is, I access something once and then slightly later, I'm going to access the same thing again. That's called temporal locality. Something used recently like to be used again. There's another called spatial locality, which means if I access something over here, something near in memory is likely to be accessed. Examples ding through an array. You access ray lement one and likely you're going to access rament two if you're scanning through the array or lots of examples like that. Let's say you're accessing a struct, and the struct is stored sequentially in memory and you access one record of destruct and maybe you're going to also access some of the other records in instruct. Let's say student ID, might be name and address, phone number, they'd all be stored near each other. These are two properties of programs that caches can take advantage of. Okay. I have this memory hierarchy diagram. I've seen lots of pictures of this. Same thing I had before, but now I'm showing increasing size going down the page and increasing speed or lower ency going up the page and they're going in opposite directions. We have registers at the top, L one cash L two cash, L three cache, main memory and disc. Now disc nowadays can be a rotating media like we were talking about earlier, a hard disk, or it can be a solid state drive. I actually drew these differently because if you still want the maximum amount of storage, I think it's still true that cost per bit that rotating media is cheaper. Solid state drives are getting pretty big, but they're not quite as big as disc can be, if you calculate cost per bit. But they're significantly faster. Solid state drives are higher in a hierarchy, maybe a little smaller but faster than disc. There are some weird things about these things. This are generally symmetric in terms of reading write performance. Rotating discs are pretty good at sequential reads and not so good at random reads because of the mechanical disc head that has to move back and forth. But if you're just streaming thing from a disc pretty fast, Slotate drives don't care that much about random versus sequential access, but they're actually asymmetric in terms of read and write bandwidth. You can read slitate drives a lot faster than you can write them because the underlying devices themselves are much faster at reading than writing. Then there's this weird thing called phase change memory, which is like there but not there. Intel made a big run out it a couple of years ago with Micron, come up with a thing called three D cross point, which is basically a phase change memory. It's another solid state device that was non volatile but faster than traditional SSD, which uses Nan technology. They basically canceled it about a year ago. I don't know what's going to happen. There's probably some other phase change memory maybe in our future. But there might be something between main memory and disc is the point. Okay. A friend of mine from HP made the slide for me a few years ago. He just tried to draw an analogy to distance. What are these hierarchies that we're dealing with? If you think about a register file, and let's say it's 1 nanosecond actually faster than that, let's say 1 nanosecond equates to 1 meter of distance. So your cache may be 10 nanoseconds, let's say ten times slower. It's maybe 10 meters away. Then our dram might be 100 times slower. Now 100 meters away. Our fabric de ram if you're talking about some uma system. Nowadays, we have things like C XL where the member is remote, maybe it's four times slower than regular dram. Maybe it's 400 meters away. Then this is when three D cross point was there. It was like 1,000 meters. It's going from here back to my office, maybe a little further. In terms of distance from 1 meter to 1,000 meters. If we talk about where is SSD, that's going from here to say Irvine. That's how much further away it is in time if time were distance. If we talk about hard disk drives, There are a lot further away, that's going from here to Boston. When your machines to read something from disc, it's significantly slower. How many know about taking operating system, know about paging, talked about paging and swapping? I know on an old computer that doesn't have enough memory, You'll see when you really start going and it's getting really slow, the disc is making crazy noises. That's because pages are moving in and out because you don't have enough memory, we'll talk about virtual memory. You don't have enough memory, it's swapping pages in and out from disc. But if each of those axises takes 5 million times longer than a normal access, that's really going to slow your machine now and that's why when you start running out of memory, machine gets really slow. It even happens on your laptop today. Programs just have a voracious appetite for memory and a lot of them have bugs where they leak memory and you start to run out of memory. Okay. All right. Fundamentals, cash hit, cash, it times time hit ratio. Miss ratio. These are just terms we use. Hit means, what we're looking for is in the cache, and Miss means what we're looking for is not in the cache. Hit time is how long does it take to access the cache, and we got a hit. That was the latency I was talking about before. Four cycles for L one is latency. It's a hit time. Mis time is how long does it take to refill the cache and get the data I want. I missed in the L one, it goes out to main memory 100 cycles away. It takes me 100 cycles. To recover or 104 cycles, if you count the original four cycle miss. Hit ratio is just the number of hits over a number of axises. I count the number of times to access the cash, how many of those were hits. I want that to be a big number. This ratio is just one minus that number. These ratios can be deceiving because your performance your program really depends on how much time you spend waiting for memory. If your program only does four load instructions, and they all miss That probably doesn't matter. Or let's say your program did four load instruction, they all hit. It probably doesn't matter because you're doing so much other work that it doesn't matter if you 100% miss rate or 100% hit rate. When we're talking about evaluating, we'll talk about this a bit. The performance of member system, sometimes we just look at raw misses. Sometimes we like to look at something like what is the misses per thousand instructions? Because that takes into account how many load of stores I'm actually doing or this could be instruction misses, but basically, how many axes am I doing? Does it matter what my hit rate is. We'll look at some metrics like that later lecture. Okay. Okay, more terminology. Cash line size or cash block size is the basic unit that the cash deals in. We try to exploit spatial locality. By loading caches with a cache block. Instead of cashing just the thing we want, we cash some of the things around it, and that unit is called the cahline size or cash block size. We have different kinds of caches, instruction caches that hold only instructions, data caches of data and unified caches that can hold either one. We'll talk about these three, the misses Cold misses, which are also called compulsory, means they have to happen. Capacity misses happen because the cache isn't big enough, and then conflict misses happened because of the design of the cache. Because you're not able to put the data where you want to in the cache, you have to evict something. Okay. So out of memory access, how do I know if something hits or misses in the cache? Or in a cache miss? Where do I put the new data? What data do I throw out? And how do I identify what these data are? So we'll talk about those. Okay. Here's a very simple cache. This cache has a box size of 16 bytes or four words, let's say a word is four bytes. He has two entries. It has this top row is one entry, and this bottom row is this entry. This is called the data array. This is where I'm going to store the cache data, the stuff I'm interested in. This is overhead, which is called the tag, and that's where I'm going to store the identifier that tells me what's in the cache. When I want to look in the cache to see if something's there or not, I'm going to look in the tag and it's going to tell me if the thing I want is either in this row or this row or this set or that set. Let's do an example and on the right, I've got main memory and I've divided it into here's the addresses going down the page. That's weird. I usually make addresses go the other way, but anyway, zero at the top, 60 at the bottom, and it's got data AEF, and I put these red lines here to mark what the cache blocks are in memory. If I take memory and divide it into blocks, that's what these red lines represent. Let's do our first access. Okay. So I'm going to access a location four. Location four happens in this first cache block. And so I want that data C, and so I'm going to go access it and put it into cache. Notice I put the tag here as zero because that's the cache that's the first address of the cache block. Now, I get another access, which is 24 and that's a miss. I look at the cache, it's not there. And so I load this line. Okay. And I put 16 there. I've got two things in my cache. If I do one more access, what's going to happen? Well, two things could happen. O two things that could happen now? Nick Right. Okay. So you're going to hit in the cash or going to miss access location eight. So this is location zero is eight. Because each of these is four bytes. That hits in this cache entry. That's great. Okay. And now we access location 48, which is not in the cache. So 48 is the bottom block and it's not block zero block 16, and now I'm going to replace somebody. Which one should I replace? The top one or the bottom one. Yeah. B that's not something not being used for a long time. Which of these should I replace? 01616 because I just hit on zero. That means I probably need to remember something. I hadn't put it in the slide here. But the tag has to have some information that says, Hey, I just use this cache line, probably it's still hot. Don't evict it if you can avoid it. I'm going to evict the bottom one. I put that data in there. Now let's do a store to address four. This data C here is going to become xx It's a hit X. Now notice what I did here, is I put this D next to that. Why did I do that? Right. Exactly. I signifies that the cash line is dirty because in the previous thing, I just evicted a cash line. I just erased it. I overwrote this bottom line. I just erased what was there before. I can't do that this one. When I evict, this modified data has to be written to Address four and made visible. Yeah. The previous miss, which one? So that miss was the first time we access this block here. So that would be compulsory. There's nothing we can do about it because if we've never accessed it before, even in infinite cash, we will have that miss. We'll talk about that maybe at the beginning of next time lecture. Compulsory misses are basically just take the stream of addresses and if the cache was infinitely big and you still have those misses, then they're compulsory. They're cold misses. Then as you make the cache smaller, the additional misses you get because the cache isn't big enough become capacity misses. That would be a fully associative cache that is smaller than infinite. Then if you now make it set associative, which we'll talk about what these things are those additional misses did become conflict misses. That's the way you do that. I think there's a homework problem like that. Okay. So now we're going to go to address 40. Now, 40 is from line 32, which I think is we haven't seen before. Okay. So I'm going to put that here. Because I recently just touched the top line, so I'm going to replace the bottom line. Now I get a miss here. I want to read 16. So the most recent line that I access is the bottom one, so I want to replace the top one. Now in order to bring in line 16, I've got to evict this one, which means I got to write it back like that, and then I'm going to read in the new line. Okay. And finally, let's do a store miss on line 52, address 52. Now, 52 is in this bottom cache block, and that was previously in the cache. This is no longer a compulsory or cold miss, Because infinite cache would still have that line. This one now becomes a capacity miss B's fully associated cash, the reason I can't have this line in there is because the fulci cash wasn't big enough. Okay. Then I store to it. That was an example of a store miss, which we'll talk about those too. In this case, I'm storing to the cash and I'm deciding to allocate a cash line on a store miss, and that's something that's a policy that we'll talk about. Any questions? Pretty fast. Yeah. This last miss was a capacity miss of this one 16 Yeah, I think so because we had that line earlier. Yeah. Yeah. Conflict misses can only happen in non fully associative caches. Okay. And yeah. Okay. Any other questions? Okay. So hopefully, I went kind of fast, but that's kind of review, hopefully, and we're going to go a little deeper into catches now. Oh, it's time for the lecture sponsor already. Today's lecture sponsor is, I really like this. This is CS's latest vocal sensation. Do you know who that is? You haven't heard this person singing? Her name is S S. S. Okay. All right. Okay. Let's talk about organization. Okay. Associativity. So associativity refers to where can a block be placed in a cash? So, I go from examples here like fully associated cash. Let's say this full associate cash has eight blocks. It can cache eight different blocks. If I want to load this line block 12 into this cache, I can choose any of these slots. I have complete freedom. I can use LRU. I can use random, I can decide where I'm going to replace it, but I can use any of them. Fully associative caches have the maximum freedom, and that's why they don't have conflict misses. Okay. The most restrictive cache is a direct map cache. I direct map cache, and we'll talk about this. I'm going to take the address modulo some index. Okay. Ate 12 mod eight. I take the block size mod eight, there's eight blocks. This block has to go into block four. There's no choice. I can get lots of conflict misses, for example, I anything that's something mod eight equals four. For example, let's say 12, what would the next one be 2028, is that right? Yeah. That's four. We also go there, so this block would also conflict. Okay. Okay. Set associated caches, say I'm going to have multiple ways. I like to call them ways, not sets. I have two blocks per set. So I can put them in zero or one, set zero or set one. Okay. I'm sorry, set zero one way zero or way one. So I by having two way set associative, I basically have four sets now instead of eight sets, but each set has two ways. Okay. So now I could accommodate 12 and 20 at the same time, but not the next block that's also mod four. That's the same residual mod four. Okay. So these are kind of the degrees of freedom, fully associate direct map and way set associative. The index is the pointer to the set. Where this might be cashed. Okay? A set is a collection of cash box with the same cash index. I have a better picture in a minute. Okay. I like this picture which I stole from Professor Tilson. Okay. You can think of a cache as three dimension. I've never really thought of it this way before. I've got a number of dimensions. The sets are indexed in the y axis here, which is how I like to think about it. The blocks is axis in the x direction. The bigger the cache line size, the wider this x dimension is. And then the sets is in the z direction. Those are the ways. A number of cache lines that have the same set can have multiple ways, multiple sci. Okay. I don't usually draw it in three dimensions, but it's interesting way to think about it. Okay. Okay. So how big a cache is, we can measure how many cash blocks it has. If I just say, well, how many clash blocks does I have, sorry, what's the size of each cash block? Times the number of blocks? That's how big the cache is. Another way to think about it is how big is each block times the number of sets times the associativity. If you go back to the three dimensional picture. Number of sets times the associativity times the size of the block gives me the volume of this storage, Okay. Okay. When we talk about cash size, just make sure when we quote cash size, we're not talking about all the overhead bits. We're just talking about how much data can you store in it, not what the tag size is or stuff like that. Yeah. This one. A number Yes. So the block offset is the amount of bits needed to specify a byte within a block. So if the cash line is 64 bytes, then that would be six bits, bit zero through five. Okay. Okay. Here's a direct map cache. Again, it has two components, it has a data array and a tag array. These are just rams. A RM typically is organized as a rocder and a column decoder. It's a two dimensional usually. If you look at a chip, you take a bunch of ram cells, you lay them out in a grid, and you have a rood row of these am I accessing and then that row is access and then I have a x at the bottom which selects which columns I want. Okay. Okay. So this isn't that different? In this case, I have one column for the Tager and have a decoder, which says, which row A wants. So in this case, let's say, I have this cache where the cash line size is 16 bytes. So bits three through zero are the bite that I want within the cache. Why? Then the index would be if there's 256 rows, that's going to be the next eight bits 11 through four. Then the tag is everything else. I take an address. I throw it on top of this rubric here, and then I look at well, this address is ABCDE 043. I take these bits zero four and I send them to the decoder and I find four. Notice I send it to two decoders, one is in the tag array, one in data array logically. Okay. Okay. What's going to happen now is I'm going to access R four, I'm going to pull out this data, ABCD and a valid bit. I send that down to the comparative here and I compare it with tag 31 through 12. If its 31 to 12, ABCDE match what's coming out of the tag, it's a hit, and it's valid. I take the output as comparative and I end it with a valid bit. If the valid bit is zero, then I ignore what's stored in the tag it's not been initialized. That makes sense. That's how you hit or miss. If it's a hit, then I additionally take bits three through two here. Let's say that the array is outputting 16 bytes. Let's see. What is that? That's 32 bits, 64, says 128 bits. It says 128 bits wide, and it brings out 32 bits at a time. I have this four to one x which is xing different 32 bit values. I use bits three through two to select which word I want out of the cache. And then if you actually want to bite and stuff like that, usually that's done in the data path in a load of liner, you have to do sign extension on sine bytes and stuff like that. Does that make sense? That's the simplest cache direct map or one way set associative, isn't that way, think about it? Okay. So if I want a set associative cache, It's the same idea, but now I've got the direct have cache, I've got a tag array and a data array, but I've got three of them. So it's a three way set associate of cache. And one cache set are all the cache lines that share the same index. So I do this purple box here. What I'm going to do is I'm going to look at all three tags at the same time. If any of them hit, I will control this x to select the data array of the cache way that hit. And if all of the miss, then I have a miss. Okay. So the set is a set of addresses with identical index bits. The number of ways is the number of cash blocks I can store that have the same index. So if there's three way set associative, that means I can accommodate up to three cash blocks in my cash that all have the same index. Remember the one way set associative, each index had to be unique. Every cash block in there had to have a unique index. Three way set associative, I can have three cash blocks that have the same index and when I get the fourth one, then I have a problem. Okay. And obviously, in a fully associated cache, this one set extends all the way out here, each tag array becomes trivially just one entry. So now I'm just doing a comparative for each entry. Does that make sense? Okay. All right. Let's do a clicker. Given a four kilobit two way sto cache with 32 bit line size and a 32 bit address, how many bits are needed for address in the tag array? Let's do this. Okay. I want to figure out what the number of blocks is first. And then I want to figure out the number sets. And that will determine how big the index is, and then I can figure out how big the tag is based on what's left over. Talk to your neighbors to compare answers. You're all over the map, so pretty even distribution. Art. Okay. All right. It's looking a little better. Not quite right yet, but we still thinking? Maybe I made a mistake. Definitely possible. I'm going to stop the poll. Let's do it together. See. So right now, we're kind of pretty evenly distributed. All right. Okay. What is the number of blocks? Well, let's figure that out. So it's four kilobyte. Divided by 32 bytes per block, right? So four k is two 12 and 32 is two to five. I can do 12 minus five to get the number of bits or two 12 minus five, which 12 minus five is seven, right? So that's 12827 is 1208. It's 128 blocks, right? How many sets are there? 64, right? Yeah, divide by two. So it's a 128/2 is 64 sets. So how many bits do I need in the index? Six. How many bits I need in the offset? Five. And this is 32, right? So 32 minus six minus five is 32 -11, which is 21, it's 20 this will be 21 bits. Do you see relationship between the associativity and the size of the tag? What happens if I increase the associativity, what would happen to the size of the tag. For the same size cache. Increase. That makes sense, right because let's say a fully associated cache is going to have zero index bits, which means that the tag is going to be all the way down to here. A direct map cache is going to have one more index bit and fewer tag bits. The complexity of the compare I have to do increases with the associativity. The tag gets wider. The wider the tag is the longer it takes to compare all the bits. Okay. What's the average associate, that's a great question. What's the average associated on a cache? I think it depends on which processor you're looking at. I used to think if you just look at Intel, AMD, mainstream processors, they're usually like two or four ways associated, the Zen core showed you before Z four, I think they're four ways associated. But when I started working at Cavium, our catch was 37 way side associate. It's crazy weird number. 37 person is a prime number. I don't know why they choose that. Secondly, it's not power of two, it's all big number. I think it depends. But mainstream processors, there's reasons for this. We'll talk about virtual and physical caches that they're usually 2-8 associative. I think an exception. I just looking the other day. I think the M one, the Mac M has 192 k instruction cache, like a gigantic e cache. It's probably fairly set associative, although I don't know if I know how set associated is. Okay. Okay. So when we have a miss, we have to decide who to replace. That's called the cache victim. When I have a direct map cache, it's super easy because there's only one set to replace. Only one set and the set only has one way. In a fully associate case, I could replace any of the blocks, and a set associated cache, I replaced any of the blocks that have the same set index. If a cash miss occurs on cash line x and x was previously evicted, this is called the conflict mix. Okay. Um. Okay. Is that right? This occurs on cash line X. X was previously evicted. Yes. Okay. The reason the reason I'm removing cash line x is because I want to make room for something else. And the reason I'm removing is because there's a conflict, right? Because I don't have enough associativity to accommodate it. Oh, there's another clicker. Let's just quick. Okay. Okay. I got to find the start the poll. Let's do like 1 minute. So what's the relative frequency of conflict misses in general from least to most in these caches, assume they're all the same size. Okay. Anyone else want to lik in? Question. Okay. So mine previously the first timeline. That makes it a cold mess. Yeah. So if cash line x is the first time I'm missing on it, then it's compulsory. Every cash will miss on it. Does that make sense? Okay. So I think I'll stop the poll here. So, the answer is D, because I have the most I have the fewest complex misses of fully associative, and then I go set associative, I've reduced the complet misses, and then if I direct map, I have the most complex misses because there's only one way to put something. Okay. Just an aside. Remember in the last homework or maybe it was slides in the homework, I showed you the R 4,000 pipeline and it had a DF DS and a TC stage. Okay. Somebody asked me, what's the TC stage? That is what we do for tag check. The R 4,000 had direct map caches. One of the advantages of a direct map cache in the microarchitecture sense is that I don't need to complete the tag check before I start using the data. Why is that different from set associate? Is set associative cache, I've got to determine which way hit. Then I had that Mux, I have a three way set associate cache. I do all the compares and I select the data I want. Okay. In a direct map cache, I can just start using the data before I even know if it's a hit or not, and that's what we used the TC stage for. So we actually started using the data in DS before you even know if the cache hit or not. And then sometime during the TC stage, we determined it was a hit or miss. I was a miss, then we backed everything up. And that's another ball of wax. But that machine when we was in order machine when we detected a miss sometimes, it was too late. The instruction had already moved forward. And so we actually had to back the pipeline backward. Yeah. Okay. Yeah, I guess the advantage of direct cash is we can start using the day before we know if it's good or not. Generally, the cashes work pretty well, and so most of the time it will be a hit, and so it's a good gamble to make. However, direct cash is that we see don't have as good data pattern. Conflict. They have more conflict misses and so our miss rate is going to be worse than it would be with set of soot. There's definitely trade offs to be made. There's other machines that did the same thing. I think in the Alpha, they had a two way set of soga cash and they had a way prediction mechanism. Before they did the tag check, they say, Well, it's probably this way based on history. Just choose way one or way zero and try to get around it that way. In the in the avian machine where we have 37 way set associated cash. The reason I was told that we had that architecture was because it actually saved power because we only had to fire up the way that hit and it worked pretty well for high cash, I suppose. Okay. So here's our cash types. We have capacity kind of misses that can happen. Capacity misses can happen in all of them. Coflict misses can only happen in set associated and direct map, and cold misses again, all cash is susceptible to that. Or cold or compulsory. This is some old data. So when we have to choose something to victimize, we have to have a replacement policy. The replacement policy and direct map is super easy. We just replace the set that we indexed. Says fully associative, we have to decide on who to get rid of. Ideally, you'd like to keep the cache item that is the next use. You have no way to know that. But if I'm loading this value A here and sometime 100 cycles later, I'm going to load A again. I'd like to keep A in the cache. But I don't know that because I don't know if I'm going to load a again. Ps have very dynamic behavior as we seem like branch prediction, we don't really know for sure, where the program is going to go. We do some other things like a recently used. Let's say I have a two way set of set of cash and I keep track of which data was least recently used or which data was most recently used, and evict the one that's least recently used. If I used it recently, I want to keep it. It's probably hot data. If I use it at least recently, it's probably maybe I'm done with it. I can get rid of it. I show you a pseudo LRU algorithm. Okay. Random is easy. We just have a random number generator, really truly random, some pseudo random number generator, and we can see this is pretty old data, but for reasonable cast size big cast sizes True LRU and random aren't that different in performance like here, 2506 kilobytg 1.15% miss rate versus 1.17 for random. For smaller cases. Actually in this case, random is better. I'm sorry, random is worse. There was one case of random is better, wasn't it? No. Here's random, the same as LRU. These are pretty small numbers, and spending a lot of logic on LRU for our 37 way set associative cash. I don't remember, but if we had some kind of PFO, we didn't do LRU was too expensive. But it didn't matter that much. In fact, on the early MIPS machines, we'll talk about TLBs TLB is another kind of cash cash is addresses. Our replacement algorithm for fully associated TLB was random. We just randomly picked an int to replace. Okay. So how do we do LRU seems to be a good way to go. Let's assume that associativity is the number of ways N, and we need to store all possible orderings of least recently used to most recently used, each represented by log two N. Okay? So if I have n ways, then the way I specify a particular way is by number. So I eight ways, then I need three bits to specify each way, zero seven. Okay. If I have eight ways and I want to keep a queue of them from most recently used, I could have a little q. The most really use three bit entry, we go here, least recently use three bit entry we go here. When everybody gets used, they go to the top of the list, everybody pushes down. Last really use will be the last one. That's one way you can implement it. Now, in theory, If you look at the number of permutations of orderings, you could say, well, if there's n of these, then n factorial is the number of permutations of possible orderings. And log two of that number, sealing of log two would be the number of possible orderings. Okay. And that's the number of orderings for each set, and then I have to do this for every set. This what I described here, this FIFO would be for every set. It's not a trivial amount of logic, and we first started designing these circuits like our 4,000 time frame, it was way too much logic to consider doing. Nowadays, you could probably throw this into Vg come up with some state machine that does this with a reasonable number of gates because we have so many gates now, you can actually compute this. Now it turns out this is not that practical because I think had an example in my notes. If you take a I factorial is 40,320. So there's 40,320 different orderings of these eight things. Then to know which one is least tu used, you'd have to decode each of those permutations, which is least used. That's quite a bit of logic. So it's actually fewer bits. If you do log two factorial times number of sets. It's fewer bits, but you have a lot of combinatorial logic to decode it. This is actually a little simpler. It's a little more bits in this case. This would be eight times log two this is three bits. So three times eight would be 24 bits. Was I think this was 16 bits. Anyway. So there's some trade off to make. Are any questions on that? All right. Okay. Okay. Yeah, this is just a math I just went through. It's 288 ways times three is 24 bits per set. The number of blocks would be 64 k divided by 64 the block side 2024. The number of sets is 120004/8, it's 128 sets, so I need about 303,000 bits or about 384 bytes, which isn't too bad for 64 kilobyte cache overhead. As long as I can do it quickly. Now that may be the challenge. How do you do this quickly? Okay. Here are the complexities. When you have a cache miss, I have to read the LRU bits and pick the LRU set B I need to know who am I going to replace. On the right on the refill, when the data comes back, I need to update the LRU bits Because now this lease use things is now the most recently used thing because I've just refilled it. It's the freshest thing. Of course, if you have a machine that's doing multiple loads and multiple cache misses at the same time, this gets fairly complicated. Now, this is okay because on a cache miss, I usually have some time. Once I determine it's a mess, everything stopped for a little while. I got to go issue the request to the next level of memory hierarchy, take some cycles. I can do all this computation. On a cache hit, I have to read the LRE bits and update the LRE bits so it's not the least recently used anymore. It's not the most recently used. Now that's problematic. Because in our pipeline, we're either reading the data cache or writing the cache, we're not reading writing the data cache in the same cycle. This could be a problem. It might be a bandwidth issue. Okay. The way you solve this is with a bunch of logic, and as a designer, when you're designing these circuits, you have to think about well, do I store these bits in ram? If I store in ram then the ram has to be dual ported. I have to have enough bamleh to keep up with reason writes on cache hits, or do I do it in discrete logic and just a whole bunch of big placing route block, and it does this with random flip flops and things like that. For 384 bytes, it might be perfectly fine to just do it in discrete logic instead of trying to build a special ram. Okay. Okay. Just an example, this is a pretty common opozation that people do back when it was probably less so today because we have so many gates that we can use, but gates are a little more precious. This is an approximation of LRU. And that's basically keep track of the of where the most recently used is and pick something that is not the most recently used, that is the furthest distance away. This can be implemented pretty simply. I have a binary tree where each node binary tree points to a child, one points to the right, one points to the right, one points to the right. Let's say the most recently used one is this one at recently used one is this one. Okay. So then I do an access. Let's say I get a hit on way three. So this is now the most recently used one, 0123. So I flip this bit here to point the opposite direction. I'm sorry. 0123. So the one on the right is the most recently used. So I flip this bit to point away from it and I flip this bit to point away from it. So now the victim is now 011. It's this one. Now let's say I get a hit on way zero. Way zero becomes the most recently used. I flip this bit away from them and I flip this away from this half subtree. Now the victim is this one. This is like pseudo LRU. I've seen it implemented in a bunch of different chips. Finally, once I miss fill on Way two, that becomes the recently used and I flip the bit that bit. This one and that one. Okay. All right. So that's just an example. Now we talk about stores. Any questions about loads replace algorithm? Okay. Stores loads are pretty easy because we just load the cache with the missing cache line. What happens on a store? I have more decisions to make. First of all, let's assume it's a store hit. We saw an example for write back cache where the cache line is updated, but the next level in the cache hierarchy or memory is not. The other policy would be, let's write through the cache. That is it hits in the cache, let's update the cache, but also let's send that data to the next level cache or main memory so that this data is consistent with the lower levels of the memory hierarchy. Okay. So right back, the stores go straight to the cache. The tag needs to maintain a D bit, a dirty bit, which says that this this cache line is now dirty with respect to the rest of the memory archy. Okay. When cash line is replaced, we need to write that back. We often call this ownership. The cash is said to own the cash line and it's responsible for making sure that the new value is visible at some point. Visible means a bunch of things. In a Uipsoran it's visible to the rest of the memory archy. In a multiprocessor means it's visible to other processors, which can be GPUs, it could be CPU, it could be IO. Actually IO, I guess we apply to Nipsor's a problem that we talked about a lot in two 40 b that deals with cash consistency. Okay. Okay. There are other policies like right through, sometimes called right around where the stores go to the cache and the next syl me hierarchy. There's no need for a dirty bit anymore because the line is always clean with respect to memory. Okay? Um. The rights generally go to a right buffer because remember, the amount of bandwidth decreases as you go out in the menor hierarchy, and the processor has a very high bandwidth for stores. I can do a store recycle or in the case of superscalar machines, it can do multiple stores per cycle. We need to keep up with that. The L one cache has a lot of bandwidth. But if the line cache isn't going to take the stores Sorry, I I'm going to send every store to the next level of cash iarchy I have a place to put it because the next level of cash iarchy probably doesn't have the same bandwidth as L one. We have these things called right buffers, which are basically places to put the rights as they're making their way out to the next level of hierarchy. What are the pros and cons of this? Okay. In a right through cache the misses don't need to write back to main memory. That's nice. And I don't have to have a dirty bit simplifies cash miss processing. In right back caches, I get a lot of bamwidth because the L one cache has a lot of bandwidth, I can retire store every cycle. In the right through case, that bamwidth goes to my right buffer and the right buffer is a finite size, and it might fill up. What's another difference between having a right buffer and a right back cache? What might be another thing that favor one over the other. Let's say I'm doing lots of stores. Yeah. It seems by doing a right you're missing out band you're having cash, which is a local member that's fast right every time bandlimited Yeah. I could stall the machine because maybe the right buffer can only take so many stores until it starts to get full, and now it's got to dribble things out to the lower bandwith L two, and I slow down. Yeah. Right. So yeah, an interesting point. So the right buffer has to buffer every store. If I'm storing the same thing over and over again, then the right back cache is to just keep updating the cache. In the right buffer, each of those stores takes an entry and it might fill up. Yeah. Okay. Okay. All right. So right buffer typically looks like this. Okay. So memory systems are generally optimized for some kind of size, and generally that matches our cash line size. So like, when I go to main memory on my computer, I usually in my cash line size, let's say, 64 bytes, the DRM is optimized to return 64 bytes. I send out one address at return 64 bytes. Um, Generally, stores are smaller than that. I don't store 64 bytes all at once. I don't have an instruction that says that my risk machine, maybe I have a store instruction who stores 64 bits. There's another factor of the memory system, which is most memory systems have error correcting codes. Here's my memory, and let's say 64 bits wide and usually has wow we're out of time. Eight bits of error checking code. What happens if I want to write 32 bits to the main memory is have to read 64 bits and eight bits of ECC into the processor. After merge of 32 bits in, generate a new error correction code and write it back because error correction code applies to all 64 bits. If I change any of these bits, the ECC has to change. Doing writes that are less than 64 bits is very slow in main memory. I like to avoid those. Typically these right buffers will have the data, a byte mass which bites I'm writing in the address and they'll do some crazy things like if they see 2302 bits things that are in the same 64 bit word, they'll merge them together. The other thing that has to do is if somebody is storing something and somebody tries to read it, I have to check the right buffer for the data because it might be sitting here in the right buffer, and that causes this basically cash like structure. Anyway, I'll cover this next time. Sorry I didn't get all the way through. Almost all the way through. You're welcome.
