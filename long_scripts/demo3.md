Name: Arturo Amaya
Lecture Name: Lecture 10
HeyGen API key: YTFiZGViYzhjZTcyNDUxNzgwMzM1MWQyNmI5ZWViNGEtMTcxNzc5MjcwMQ==
Default Composition: [type:pip]
Default Transition: {type: fade, duration: 1.0}
Default Avatar: (id:Luke_public_3_20240306, voice_id:5dddee02307b4f49a17c123c120a60ca, position:0.75;0.75, scale:0.5, style:closeUp, cbc:#453423, bc:#FFEE22)
Slides:
    https://drive.google.com/file/d/1LCVuCQfYl8j8a_bhRICgLwIk0Qj-JqbX/view?usp=share_link
    https://drive.google.com/file/d/1--tozSXXEnJ6pI7JBk8zBx2js4Byoo1y/view?usp=share_link
    https://drive.google.com/file/d/1IEsHWg4SD4VnoFjQfPiIOqYszoAsrFA4/view?usp=share_link
    https://drive.google.com/file/d/1QAV3VwCKP-CBXjyZGdIT14oXYpc7wCEe/view?usp=share_link
    https://drive.google.com/file/d/1QAV3VwCKP-CBXjyZGdIT14oXYpc7wCEe/view?usp=share_link
--

(position:0.6;0.75, scale:0.9) Okay. So the next machine we look at is the Sn one. This is a Sn two, I can't remember. S two. Son came out of a project at Stanford, stands for Stanford University Network. It was started by Andy Bekelhin when he was a grad student, and he started this company called Sn Microsystems along with Bill Joy from Berkeley, who wrote BSDUix and also VI If you use VM that was Bill Joy. Anyway, they started this company. They came on this computer. And it used the same processor as the Mac. It actually had a 68,000 processor. But because of the work station wanted to go fast, they built a unified cache outside the chip made of static ram. We'll talk about unified caches and was virtually addressed, there's all sorts interesting things about it. Anyway, that was about 1985, so only a year after the MAC. This machine costs quite a bit more than the Mac, and MAC costs $400. This is probably like $10,000, which was cheap enough for engineers to get at their work, but not cheap enough that you buy it for your home. 

Okay. And then we go to something like fairly recent This is AMD Zen four, which they just announced the Zen five. So it's got 1 megabyte. I think the cash I just showed you in the S one, S two was probably 16 k. Might have been 64 k. It was pretty big actually for the time because it was an expensive machine. But this machine has 1 megabyte of L two, 32 megabytes of L three. I mean, the original Mac had 64 kilobytes of main memory. It's main memory is smaller than the L three cache of a modern processor. Then it's got IND cache 32 k. There's a bug on this slide. It says load, it's going this way, stores going that way, but I think those arrows are wrong. I guess this is AMD marketing. Anyway. There's a bunch of details here, which we'll talk about later.

(scale: 0.8) Okay, what is the principle behind all these caches? The principle is something called memory locality. That's the principle that a future memory access are near past accesses, and they can be near in two ways. They can be near in time. That is, I access something once and then slightly later, I'm going to access the same thing again. That's called temporal locality. Something used recently like to be used again. There's another called spatial locality, which means if I access something over here, something near in memory is likely to be accessed. Examples ding through an array. You access ray lement one and likely you're going to access rament two if you're scanning through the array or lots of examples like that. Let's say you're accessing a struct, and the struct is stored sequentially in memory and you access one record of destruct and maybe you're going to also access some of the other records in instruct. Let's say student ID, might be name and address, phone number, they'd all be stored near each other. These are two properties of programs that caches can take advantage of. 

{circlecrop} (position: 0.75;0.25, scale:0.5, style: circle, cbc: #FFFFFF) Okay. I have this memory hierarchy diagram. I've seen lots of pictures of this. Same thing I had before, but now I'm showing increasing size going down the page and increasing speed or lower ency going up the page and they're going in opposite directions. We have registers at the top, L one cash L two cash, L three cache, main memory and disc. Now disc nowadays can be a rotating media like we were talking about earlier, a hard disk, or it can be a solid state drive. I actually drew these differently because if you still want the maximum amount of storage, I think it's still true that cost per bit that rotating media is cheaper. Solid state drives are getting pretty big, but they're not quite as big as disc can be, if you calculate cost per bit. But they're significantly faster. Solid state drives are higher in a hierarchy, maybe a little smaller but faster than disc. There are some weird things about these things. This are generally symmetric in terms of reading write performance. 

(position: 0.75;0.25, scale: 0.5, style: circle, cbc: #FFFFFF) Rotating discs are pretty good at sequential reads and not so good at random reads because of the mechanical disc head that has to move back and forth. But if you're just streaming thing from a disc pretty fast, Slotate drives don't care that much about random versus sequential access, but they're actually asymmetric in terms of read and write bandwidth. You can read slitate drives a lot faster than you can write them because the underlying devices themselves are much faster at reading than writing. Then there's this weird thing called phase change memory, which is like there but not there. Intel made a big run out it a couple of years ago with Micron, come up with a thing called three D cross point, which is basically a phase change memory. It's another solid state device that was non volatile but faster than traditional SSD, which uses Nan technology. They basically canceled it about a year ago. I don't know what's going to happen. There's probably some other phase change memory maybe in our future. But there might be something between main memory and disc is the point.