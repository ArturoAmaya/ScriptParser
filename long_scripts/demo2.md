Name: Arturo Amaya
Lecture Name: Lecture 10
HeyGen API key: NTYwZDhlZDhiZWIxNDUyNGE4OWU2NmVhZjBlNmFlZWItMTcxNzc5MjQ3OQ==
Default Composition:[type:pip]
Default Transition: {type: fade, duration: 1.0}
Default Avatar: (id:Luke_public_3_20240306, voice_id:5dddee02307b4f49a17c123c120a60ca, position:0.75;0.75, scale:0.5, style:closeUp, cbc:#453423, bc:#FFEE22)
Slides:
    https://drive.google.com/file/d/1ph-ge8GzqtAKVwhDvzH0qOZ85FM2oToY/view?usp=share_link
    https://drive.google.com/file/d/1if3ZwjKoIuMX07Ql2kUr--x-EgDA6ttE/view?usp=share_link
    https://drive.google.com/file/d/1if3ZwjKoIuMX07Ql2kUr--x-EgDA6ttE/view?usp=share_link
    https://drive.google.com/file/d/1mzp5_bGDtnXuuvbp_T_47EdWhB5mH8lF/view?usp=share_link

--

[type:voiceover] So it's a optimizing for different things. Memory is taking advantage of sorry, computer is taking advantage of More's law by having more capability and memory is taking care of getting more transistors to have more storage, right? So we're both actually using More's law. It's just that we're benefiting in different ways. And that's the way it is, right? Because to talk to memory, we need to do communication has been pointed out, the bigger the array, the more communication we have to do to get to individual cell, the more you have There's always a trade off in memory, right? When you want to make memory fast, you make very small arrays with a lot of decoders and a lot of sense amps and all this crazy stuff. If you want to make the memory dense, you make bigger arrays, and now it takes longer to decode the address, it takes longer to sense the wires, it takes longer, everything takes longer. If you look at back at processors in 1980, they had some examples that basically didn't have to have cash memory because the processor runs at a certain rate, the memory runs a certain rate. When you go to access memory takes a couple of cycles, you get the data back. Then as this gap started to get bigger and the processors got faster, and the memory latency didn't improve, then we start to have to integrate faster memory. Smaller memories that are faster,s always a trade off, the smaller the memory the faster, the bigger memory, the slower. I can create these caches, which are smaller and take advantage of things we're going to talk about hopefully it's review of fast memory, keeping things close to the processor and only accessing slow memory when I need to. 1980, no integrated cache. RMS thousand circa 2000 was the first chip to have integrated L one and L two. We put L one and L two caches on the chip because of this increasing gap. If it here in 2000, the gap is already getting pretty big. Then I'll show you examples of other processors that do even more. 

{slideleft}(position: 0.75;0.25, scale:0.5, style: circle, cbc: #FFFFFF) Some basics with caches. Hopefully, this is a review. I'm going to go a little fast, please ask questions or stop me if I'm going too fast because we've all some of different computer architecture background. Compute one fast memory and physics makes bigger memory is slower. That's just the way the physics we live in. Bigger memory means longer wires, wires are slow. Smaller transistors get high density. For example, if I want really high density, use dynamic random axis memory where each bit is stored basically in one transistor or one capacitor. How many are familiar with DRM? D RAM is the memory that you have mostly in your computer. It needs to be refreshed. Every now and then, every now and then every couple of milliseconds or so, your computer is actually reading every cell and refreshing its charge because if it didn't do that, the charge would leak away and you lose data. Also, when you turn the power off, you lose all data because the charges leaks away. Rams are built for multiple transistors. If you took one 40 from me, you've seen these 60 cells cross coupled inverters with a couple of axis transistors, significantly bigger than one transistor. This is not as dense but faster. Architects create hierarchies of memory to trade off speed and size. If I want lots of size, I'm going to have a slow memory, and if I want speed, I'm going to have a smaller memory that's faster. Here's the hierarchies we have and lots of examples, but here's this one. The fastest memory in a computer is usually the register file. That's usually accessible in one cycle. Not all cases, but most machines can access reg one cycle as we've seen in our pipeline. The L one cache and modern machines is typically about four cycles of latency. Then L two is typically like 12 cycles, L three might be like 60 to 100 cycles, and DRM might be anywhere from hundreds of cycles. This could be DDR four, could be HBM memory. It's still significantly slower in latency.

{concat}(position: 0.75;0.25)The other thing that happens is the bandwidth So with high performance with low latency, I get a lot of bamwidth Register file has an extreme amount of bamwid. As we'll see when we do out of order or superscalar machines. Some of these register files have 12 or 15 things I can read in one cycle. So there's a lot of data, and I can similarly I can write five or six things in the same cycle. L one cache typically has an order of gigabytes per second of bandwidth. Then the bandwidth decreases as we go down the hierarchy here. That makes sense. Going further and further away, wires get more expensive, so I'm not going to be able to afford as many wires to get there. Equation change a little bit with HPM with HPM memory, which is die attached on top of the die. Typically they can't afford to have wider interfaces. But generally, that's a trick. You can see all this stuff if you just do CT PxyPU info on your computer, you'll see what kind of computer you have and you can look up all of the different characteristics of how many caches it has and what size they are, stuff like that.

Okay. A bit of history. Here's an evolution of caches. This was the first Mctshme out in 1984. I might have told this story. This is the first machine, not the first machine, but the first commercially successful machine that integrated desktop graphics. Stuff, we assume take for granted today, Window managers, point and click interfaces, mice. This is the first machine to really productize that. It was actually developed at Zerox Park. Steve Jobs went there, got these great ideas from Zerox basically integrated them the Apple and Zerox was never able to capitalize off it. Um, just reading this morning, actually. So actually, the first Windows operating system came out before the MAC. When Bill Gates was down at Apple and Steve Job was showing the Mac. At that time, Microsoft and Apple are actually pretty close together because Microsoft was writing software and they wanted Microsoft to write software for the Mac. So they showed him the mac. Bill Gates says, that's amazing. But before they ink the deal between Microsoft and Apple to write software for the Mac, Steve Jobs said, Well, I want to make sure that we have a head start. I want to if you sign this contract, then you cannot develop a graphical user interface for window for PCs until one year after the machine ships. 

[type:avatar-only](position:0.5;0.5, scale: 1.0, bc:#FFC0CB, cbc:#FFFFFF) At the time, Steve Jobs like, Oh, this machine is going to ship in 1981. We have exclusive rights on this desktop metaphor, windows, mice, all that stuff until 1982. This machine did actually ship until 1984 because Steve Jobs was wrong about the schedule, and the contract said, 1982, Windows actually came out before this, even though it wasn't as good. And according to the book I'm reading Steve Jobs was furious. But there's nothing you could do because that's what he signed the deal. Anyway. Actually, the MAC interface was much better than Windows 1.0. If you ever go back, you probably find out Internet somewhere windows looked like. It was pretty bad. Okay. Okay. Apple Macintosh, 1984, no cash. It had a 68,000 microprocessor right here. I talked to random access memory, and I talked to de read only memory. The operating system was actually read only memory. When you boot the computer up, it would come up pretty fast. You didn't have to stick a floppy in there. You didn't have to have a disk drive. Most of the critical routines ran out of read only memory. 